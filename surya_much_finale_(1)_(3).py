# -*- coding: utf-8 -*-
"""Surya_much_finale (1) (3).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1IkyK7V9J6I-Ixzh3dkVpZz0bCXgRXEC2
"""

import zipfile as zf
files = zf.ZipFile("RSICD_images.zip", 'r')
files.extractall('RSICD_image')
files.close()

# # Cell 0 (Add this before Cell 1, or run it in your terminal)
!pip install evaluate
!pip install transformers # Ensure transformers is also up-to-date
!pip install accelerate # Often a dependency for transformers, good to have
!pip install datasets # Often useful for data handling in HF ecosystem
!pip install torch torchvision torchaudio
!pip install nltk
!pip install scikit-learn
!pip install pycocoevalcap

# For COCO-related metrics (CIDEr, SPICE):
!pip install pycocotools
!git clone https://github.com/tylin/coco-caption.git

# Add coco-caption/pycocoevalcap to your Python path
import sys
if 'coco-caption/pycocoevalcap' not in sys.path:
    sys.path.append('coco-caption/pycocoevalcap')
    print("Added 'coco-caption/pycocoevalcap' to Python path.")

# You may also need to install nltk data for some tokenizers used by pycocoevalcap
import nltk
try:
    nltk.download('punkt', quiet=True)
    nltk.download('wordnet', quiet=True)
    nltk.download('punkt_tab', quiet=True)
    print("NLTK data downloaded successfully")
except:
    print("NLTK download failed, but continuing...")

# Import all necessary libraries
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader, Dataset
from torchvision import transforms
from PIL import Image
import json
import os
import numpy as np
import matplotlib.pyplot as plt
from transformers import (
    ViTImageProcessor,
    ViTModel,
    AutoTokenizer,
    GPT2LMHeadModel,
    get_scheduler
)
from tqdm.auto import tqdm
import evaluate
from collections import defaultdict
import warnings
warnings.filterwarnings('ignore')

# For evaluation metrics (CIDEr and SPICE)
try:
    from pycocotools.coco import COCO
    from pycocoevalcap.eval import COCOEvalCap
    print("COCO evaluation tools imported successfully")
except ImportError:
    print("Warning: COCO evaluation tools not available. Install pycocotools and coco-caption")

# Set device and random seeds for reproducibility
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
torch.manual_seed(42)
np.random.seed(42)
if torch.cuda.is_available():
    torch.cuda.manual_seed(42)

print(f"Using device: {device}")
print(f"PyTorch version: {torch.__version__}")

# # Define image directory and caption file
# IMAGE_DIR = '/content/image'
# CAPTION_FILE = '/content/new_dataset_rscid.json'

# CELL 2: Configuration and Paths
# ============================================================================

# Configuration parameters
FIXED_CONFIG = {
    'IMAGE_DIR': '/content/RSICD_image/RSICD_images',
    'CAPTION_FILE': '/content/dataset_rsicd.json',
    'BATCH_SIZE': 8,  # Reduced batch size
    'LEARNING_RATE': 5e-5,  # Lower learning rate
    'NUM_EPOCHS': 15,  # More epochs
    'MAX_CAPTION_LENGTH': 100,
    'MAX_GENERATION_LENGTH': 30,  # Shorter generation length
    'PATIENCE': 3,  # More patience
    'VIT_MODEL': "google/vit-base-patch16-224",
    'GPT2_MODEL': "gpt2",
    'MODEL_SAVE_PATH': "best_image_captioning_model.pth",
    'WARMUP_RATIO': 0.1,
    'WEIGHT_DECAY': 0.01,
    'GRADIENT_CLIP_NORM': 1.0
}

# Create output directories
os.makedirs('outputs', exist_ok=True)
os.makedirs('plots', exist_ok=True)

print("Configuration loaded:")
for key, value in FIXED_CONFIG.items():
    print(f"  {key}: {value}")

# ============================================================================
# CELL 3: Dataset Splitting and JSON File Creation
# ============================================================================

def create_split_datasets(caption_file, output_dir='./'):
    """
    Create train/val/test splits from the original dataset and save as separate JSON files
    This implements the "create json file when split" requirement
    """
    print("=" * 60)
    print("CREATING DATASET SPLITS AND JSON FILES")
    print("=" * 60)

    # Load the original dataset JSON
    print(f"Loading dataset from: {caption_file}")
    with open(caption_file, 'r') as f:
        data = json.load(f)

    print(f"Total images in dataset: {len(data['images'])}")

    # Initialize split dictionaries in COCO format
    split_data = {
        "train": {
            "images": [],
            "annotations": [],
            "info": {"description": "RSICD Training Set"},
            "licenses": []
        },
        "val": {
            "images": [],
            "annotations": [],
            "info": {"description": "RSICD Validation Set"},
            "licenses": []
        },
        "test": {
            "images": [],
            "annotations": [],
            "info": {"description": "RSICD Test Set"},
            "licenses": []
        }
    }

    # Counters
    split_counts = {"train": 0, "val": 0, "test": 0}
    annotation_id_counter = 0

    # Process each image in the dataset
    for item in data['images']:
        split = item['split']
        split_counts[split] += 1

        # Create image entry for COCO format
        image_entry = {
            "id": len(split_data[split]["images"]),
            "width": 256,  # Default size for RSICD
            "height": 256,
            "file_name": item['filename'],
            "original_id": item['imgid']
        }
        split_data[split]["images"].append(image_entry)

        # Create annotation entries for each caption
        for sentence in item['sentences']:
            annotation_entry = {
                "id": annotation_id_counter,
                "image_id": image_entry['id'],
                "caption": sentence['raw'].strip(),
                "tokens": sentence['tokens'] if 'tokens' in sentence else sentence['raw'].split()
            }
            split_data[split]["annotations"].append(annotation_entry)
            annotation_id_counter += 1

    # Save each split to a separate JSON file
    saved_files = {}
    for split_name, split_content in split_data.items():
        output_filename = os.path.join(output_dir, f"rsicd_{split_name}.json")
        with open(output_filename, 'w') as f:
            json.dump(split_content, f, indent=2)
        saved_files[split_name] = output_filename

        print(f"✓ Saved {split_name} split:")
        print(f"  - File: {output_filename}")
        print(f"  - Images: {len(split_content['images'])}")
        print(f"  - Annotations: {len(split_content['annotations'])}")

    print(f"\nTotal split distribution:")
    for split, count in split_counts.items():
        print(f"  {split}: {count} images")

    return saved_files, split_data

# Create the dataset splits
split_files, split_data = create_split_datasets(FIXED_CONFIG['CAPTION_FILE'])

# ============================================================================
# CELL 4: Dataset Class Implementation
# ============================================================================

class RSICDDataset(Dataset):
    """
    RSICD Dataset class for loading images and captions
    """
    def __init__(self, json_file, img_dir, processor, tokenizer, split_type="train", max_length=100):
        print(f"Initializing {split_type} dataset from {json_file}")

        self.img_dir = img_dir
        self.processor = processor
        self.tokenizer = tokenizer
        self.max_length = max_length
        self.split_type = split_type

        # Set pad token
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token

        # Load data from JSON file
        with open(json_file, 'r') as f:
            self.data = json.load(f)

        # Create mappings
        self.images = self.data['images']
        self.annotations = self.data['annotations']

        # Create image_id to captions mapping
        self.image_id_to_captions = defaultdict(list)
        for ann in self.annotations:
            self.image_id_to_captions[ann['image_id']].append(ann['caption'])

        # Create image_id to filename mapping
        self.image_id_to_filename = {img['id']: img['file_name'] for img in self.images}

        # Create samples list for iteration (image_filename, caption pairs)
        self.samples = []
        for ann in self.annotations:
            img_filename = self.image_id_to_filename[ann['image_id']]
            self.samples.append((img_filename, ann['caption'], ann['image_id']))

        print(f"✓ Loaded {len(self.samples)} samples from {len(self.images)} unique images")

    def __len__(self):
        return len(self.samples)

    def __getitem__(self, idx):
        img_filename, caption, image_id = self.samples[idx]
        img_path = os.path.join(self.img_dir, img_filename)

        # Load and process image
        try:
            image = Image.open(img_path).convert("RGB")
        except Exception as e:
            print(f"Warning: Error loading image {img_path}: {e}")
            # Create a black placeholder image
            image = Image.new('RGB', (224, 224), (0, 0, 0))

        # Process image for ViT
        pixel_values = self.processor(images=image, return_tensors="pt").pixel_values.squeeze(0)

        # Tokenize caption for GPT-2
        caption_encoding = self.tokenizer(
            caption,
            max_length=self.max_length,
            padding="max_length",
            truncation=True,
            return_tensors="pt"
        )
        caption_tokens = caption_encoding.input_ids.squeeze(0)

        return {
            'pixel_values': pixel_values,
            'caption_tokens': caption_tokens,
            'caption_text': caption,
            'image_filename': img_filename,
            'image_id': image_id
        }

    def get_image_captions(self, image_id):
        """Get all captions for a specific image"""
        return self.image_id_to_captions[image_id]

# ============================================================================
# CELL 5: ViT-GPT2 Model Implementation
# ============================================================================

# Fixed ViT-GPT2 Model Implementation

class ViTGPT2Model(nn.Module):
    """
    Fixed ViT + GPT-2 Model for Image Captioning
    """
    def __init__(self, vit_model_name="google/vit-base-patch16-224", gpt2_model_name="gpt2"):
        super(ViTGPT2Model, self).__init__()

        print("Initializing ViT-GPT2 Model...")

        # Load pre-trained models
        self.vit = ViTModel.from_pretrained(vit_model_name)
        self.gpt2 = GPT2LMHeadModel.from_pretrained(gpt2_model_name)
        self.processor = ViTImageProcessor.from_pretrained(vit_model_name)
        self.tokenizer = AutoTokenizer.from_pretrained(gpt2_model_name)

        # Set pad token for GPT-2
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
        if self.gpt2.config.pad_token_id is None:
            self.gpt2.config.pad_token_id = self.gpt2.config.eos_token_id

        # Add special tokens for image captioning
        special_tokens = {
            "bos_token": "<|startoftext|>",
            "additional_special_tokens": ["<|image|>"]
        }
        self.tokenizer.add_special_tokens(special_tokens)
        self.gpt2.resize_token_embeddings(len(self.tokenizer))

        # Store special token IDs
        self.bos_token_id = self.tokenizer.bos_token_id
        self.image_token_id = self.tokenizer.convert_tokens_to_ids("<|image|>")

        # Projection layer to map ViT features to GPT-2 hidden dimension
        self.image_projection = nn.Sequential(
            nn.Linear(self.vit.config.hidden_size, self.gpt2.config.hidden_size),
            nn.LayerNorm(self.gpt2.config.hidden_size),
            nn.Dropout(0.1)
        )

        # Use global average pooling for image features
        self.image_pooling = nn.AdaptiveAvgPool1d(1)

        print(f"✓ ViT hidden size: {self.vit.config.hidden_size}")
        print(f"✓ GPT-2 hidden size: {self.gpt2.config.hidden_size}")
        print(f"✓ Vocabulary size: {len(self.tokenizer)}")

    def forward(self, pixel_values, caption_tokens=None):
        batch_size = pixel_values.shape[0]
        device = pixel_values.device

        # Extract image features using ViT
        vit_outputs = self.vit(pixel_values=pixel_values)
        image_features = vit_outputs.last_hidden_state  # (batch_size, seq_len, hidden_size)

        # Pool image features to single token
        pooled_features = self.image_pooling(image_features.transpose(1, 2)).transpose(1, 2)
        # pooled_features shape: (batch_size, 1, hidden_size)

        # Project to GPT-2 hidden dimension
        image_embeds = self.image_projection(pooled_features)

        if caption_tokens is not None:
            # Training mode
            # Prepend image token to caption
            image_token_ids = torch.full((batch_size, 1), self.image_token_id,
                                       dtype=torch.long, device=device)
            combined_tokens = torch.cat([image_token_ids, caption_tokens], dim=1)

            # Get text embeddings for all tokens
            text_embeds = self.gpt2.transformer.wte(combined_tokens)

            # Replace image token embeddings with actual image features
            text_embeds[:, 0:1, :] = image_embeds

            # Create attention mask
            image_mask = torch.ones(batch_size, 1, dtype=torch.long, device=device)
            text_mask = (caption_tokens != self.tokenizer.pad_token_id).long()
            attention_mask = torch.cat([image_mask, text_mask], dim=1)

            # Create labels for language modeling (ignore image token in loss)
            image_labels = torch.full((batch_size, 1), -100, dtype=torch.long, device=device)
            combined_labels = torch.cat([image_labels, caption_tokens], dim=1)

            # Forward pass through GPT-2
            outputs = self.gpt2(
                inputs_embeds=text_embeds,
                attention_mask=attention_mask,
                labels=combined_labels
            )

            return outputs
        else:
            # Inference mode - return image embeddings
            return image_embeds

    def generate_caption(self, pixel_values, max_length=50, num_beams=3,
                        do_sample=False, temperature=1.0, top_p=0.9):
        """
        Fixed caption generation method
        """
        self.eval()
        device = pixel_values.device
        batch_size = pixel_values.shape[0]

        with torch.no_grad():
            # Get image embeddings
            image_embeds = self.forward(pixel_values)

            generated_captions = []

            for i in range(batch_size):
                current_image_embed = image_embeds[i:i+1]  # (1, 1, hidden_size)

                # Start with BOS token instead of EOS
                if self.bos_token_id is not None:
                    start_token = self.bos_token_id
                else:
                    start_token = self.image_token_id

                input_ids = torch.tensor([[start_token]], device=device)
                generated_tokens = []

                # Create initial embeddings
                current_embeds = torch.cat([
                    current_image_embed,
                    self.gpt2.transformer.wte(input_ids)
                ], dim=1)

                for step in range(max_length):
                    # Forward pass
                    outputs = self.gpt2(inputs_embeds=current_embeds)
                    next_token_logits = outputs.logits[0, -1, :]

                    # Apply temperature and sampling
                    if do_sample and temperature > 0:
                        next_token_logits = next_token_logits / temperature

                        # Apply top-p filtering
                        if top_p < 1.0:
                            sorted_logits, sorted_indices = torch.sort(next_token_logits, descending=True)
                            cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)
                            sorted_indices_to_remove = cumulative_probs > top_p
                            sorted_indices_to_remove[1:] = sorted_indices_to_remove[:-1].clone()
                            sorted_indices_to_remove[0] = 0
                            indices_to_remove = sorted_indices[sorted_indices_to_remove]
                            next_token_logits[indices_to_remove] = float('-inf')

                        probs = F.softmax(next_token_logits, dim=-1)
                        next_token = torch.multinomial(probs, num_samples=1)
                    else:
                        next_token = torch.argmax(next_token_logits, dim=-1, keepdim=True)

                    # Stop if EOS token is generated
                    if next_token.item() == self.tokenizer.eos_token_id:
                        break

                    # Skip pad tokens
                    if next_token.item() == self.tokenizer.pad_token_id:
                        continue

                    generated_tokens.append(next_token.item())

                    # Update embeddings for next iteration
                    next_token_embed = self.gpt2.transformer.wte(next_token.unsqueeze(0))
                    current_embeds = torch.cat([current_embeds, next_token_embed], dim=1)

                    # Truncate if sequence gets too long
                    if current_embeds.shape[1] > 100:
                        current_embeds = current_embeds[:, -50:, :]  # Keep last 50 tokens

                # Decode generated tokens
                if generated_tokens:
                    caption = self.tokenizer.decode(generated_tokens, skip_special_tokens=True)
                    caption = caption.strip()
                else:
                    caption = "a satellite image"  # Fallback caption

                generated_captions.append(caption)

        return generated_captions

# ============================================================================
# CELL 6: Initialize Datasets and DataLoaders
# ============================================================================

print("=" * 60)
print("INITIALIZING DATASETS AND DATALOADERS")
print("=" * 60)

# Initialize processors and tokenizers
image_processor = ViTImageProcessor.from_pretrained(FIXED_CONFIG['VIT_MODEL'])
tokenizer = AutoTokenizer.from_pretrained(FIXED_CONFIG['GPT2_MODEL'])
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token

# Create datasets
train_dataset = RSICDDataset(
    json_file="rsicd_train.json",
    img_dir=FIXED_CONFIG['IMAGE_DIR'],
    processor=image_processor,
    tokenizer=tokenizer,
    split_type="train",
    max_length=FIXED_CONFIG['MAX_CAPTION_LENGTH']
)

val_dataset = RSICDDataset(
    json_file="rsicd_val.json",
    img_dir=FIXED_CONFIG['IMAGE_DIR'],
    processor=image_processor,
    tokenizer=tokenizer,
    split_type="val",
    max_length=FIXED_CONFIG['MAX_CAPTION_LENGTH']
)

test_dataset = RSICDDataset(
    json_file="rsicd_test.json",
    img_dir=FIXED_CONFIG['IMAGE_DIR'],
    processor=image_processor,
    tokenizer=tokenizer,
    split_type="test",
    max_length=FIXED_CONFIG['MAX_CAPTION_LENGTH']
)

# Create data loaders
def collate_fn(batch):
    """Custom collate function for batching"""
    pixel_values = torch.stack([item['pixel_values'] for item in batch])
    caption_tokens = torch.stack([item['caption_tokens'] for item in batch])
    caption_texts = [item['caption_text'] for item in batch]
    image_filenames = [item['image_filename'] for item in batch]
    image_ids = [item['image_id'] for item in batch]

    return {
        'pixel_values': pixel_values,
        'caption_tokens': caption_tokens,
        'caption_texts': caption_texts,
        'image_filenames': image_filenames,
        'image_ids': image_ids
    }

train_dataloader = DataLoader(
    train_dataset,
    batch_size=FIXED_CONFIG['BATCH_SIZE'],
    shuffle=True,
    collate_fn=collate_fn,
    num_workers=2,
    pin_memory=True if torch.cuda.is_available() else False
)

val_dataloader = DataLoader(
    val_dataset,
    batch_size=FIXED_CONFIG['BATCH_SIZE'],
    shuffle=False,
    collate_fn=collate_fn,
    num_workers=2,
    pin_memory=True if torch.cuda.is_available() else False
)

test_dataloader = DataLoader(
    test_dataset,
    batch_size=FIXED_CONFIG['BATCH_SIZE'],
    shuffle=False,
    collate_fn=collate_fn,
    num_workers=2,
    pin_memory=True if torch.cuda.is_available() else False
)

print(f"✓ Training samples: {len(train_dataset)}")
print(f"✓ Validation samples: {len(val_dataset)}")
print(f"✓ Test samples: {len(test_dataset)}")
print(f"✓ Training batches: {len(train_dataloader)}")
print(f"✓ Validation batches: {len(val_dataloader)}")
print(f"✓ Test batches: {len(test_dataloader)}")

pip install rouge_score

# ============================================================================
# CELL 7: Evaluation Metrics Implementation
# ============================================================================

class EvaluationMetrics:
    """
    Comprehensive evaluation metrics for image captioning
    This implements: BLEU 1-4, METEOR, CIDEr, ROUGE_L, SPICE
    """
    def __init__(self):
        print("Initializing evaluation metrics...")

        # Initialize Hugging Face metrics
        try:
            self.bleu_metric = evaluate.load("bleu")
            self.meteor_metric = evaluate.load("meteor")
            self.rouge_metric = evaluate.load("rouge")
            print("✓ Hugging Face metrics loaded")
        except Exception as e:
            print(f"Warning: Could not load some Hugging Face metrics: {e}")
            self.bleu_metric = None
            self.meteor_metric = None
            self.rouge_metric = None

    def compute_bleu_scores(self, predictions, references):
        """Compute BLEU 1-4 scores"""
        bleu_scores = {'bleu_1': 0, 'bleu_2': 0, 'bleu_3': 0, 'bleu_4': 0}

        if self.bleu_metric is None:
            return bleu_scores

        try:
            # BLEU-1
            bleu_1 = self.bleu_metric.compute(
                predictions=predictions,
                references=references,
                max_order=1
            )['bleu']

            # BLEU-2
            bleu_2 = self.bleu_metric.compute(
                predictions=predictions,
                references=references,
                max_order=2
            )['bleu']

            # BLEU-3
            bleu_3 = self.bleu_metric.compute(
                predictions=predictions,
                references=references,
                max_order=3
            )['bleu']

            # BLEU-4
            bleu_4 = self.bleu_metric.compute(
                predictions=predictions,
                references=references,
                max_order=4
            )['bleu']

            bleu_scores = {
                'bleu_1': bleu_1,
                'bleu_2': bleu_2,
                'bleu_3': bleu_3,
                'bleu_4': bleu_4
            }

        except Exception as e:
            print(f"Error computing BLEU scores: {e}")

        return bleu_scores

    def compute_meteor_score(self, predictions, references):
        """Compute METEOR score"""
        if self.meteor_metric is None:
            return 0.0

        try:
            meteor_score = self.meteor_metric.compute(
                predictions=predictions,
                references=references
            )['meteor']
            return meteor_score
        except Exception as e:
            print(f"Error computing METEOR score: {e}")
            return 0.0

    def compute_rouge_l_score(self, predictions, references):
        """Compute ROUGE-L score"""
        if self.rouge_metric is None:
            return 0.0

        try:
            rouge_score = self.rouge_metric.compute(
                predictions=predictions,
                references=references,
                rouge_types=['rougeL']
            )['rougeL']
            return rouge_score
        except Exception as e:
            print(f"Error computing ROUGE-L score: {e}")
            return 0.0

    def compute_coco_metrics(self, predictions_dict, references_dict):
        """
        Compute CIDEr and SPICE using COCO evaluation tools
        predictions_dict: {image_id: generated_caption}
        references_dict: {image_id: [reference_captions]}
        """
        metrics = {'cider': 0.0, 'spice': 0.0}

        try:
            # Prepare data in COCO format
            # Ground truth annotations
            ground_truth = {
                "annotations": [],
                "images": [],
                "info": {"description": "Ground truth captions"},
                "licenses": []
            }

            # Results (predictions)
            results = []

            ann_id = 0
            for image_id, ref_captions in references_dict.items():
                # Add image info
                ground_truth["images"].append({
                    "id": image_id,
                    "width": 256,
                    "height": 256,
                    "file_name": f"image_{image_id}.jpg"
                })

                # Add reference annotations
                for ref_caption in ref_captions:
                    ground_truth["annotations"].append({
                        "id": ann_id,
                        "image_id": image_id,
                        "caption": ref_caption.strip()
                    })
                    ann_id += 1

                # Add prediction
                if image_id in predictions_dict:
                    results.append({
                        "image_id": image_id,
                        "caption": predictions_dict[image_id].strip()
                    })

            # Save temporary files
            gt_file = "temp_gt.json"
            res_file = "temp_res.json"

            with open(gt_file, 'w') as f:
                json.dump(ground_truth, f)

            with open(res_file, 'w') as f:
                json.dump(results, f)

            # Evaluate using COCO tools
            coco = COCO(gt_file)
            coco_result = coco.loadRes(res_file)
            coco_eval = COCOEvalCap(coco, coco_result)
            coco_eval.params['image_id'] = coco_result.getImgIds()
            coco_eval.evaluate()

            # Extract metrics
            metrics['cider'] = coco_eval.eval.get('CIDEr', 0.0)
            metrics['spice'] = coco_eval.eval.get('SPICE', 0.0)

            # Clean up temporary files
            os.remove(gt_file)
            os.remove(res_file)

        except Exception as e:
            print(f"Error computing COCO metrics: {e}")

        return metrics

    def evaluate_model(self, model, dataloader, max_samples=None):
        """
        Comprehensive evaluation of the model
        """
        print("Starting comprehensive evaluation...")

        model.eval()
        predictions = []
        references = []
        predictions_dict = {}
        references_dict = {}

        sample_count = 0
        max_samples = max_samples or len(dataloader.dataset)

        with torch.no_grad():
            for batch_idx, batch in enumerate(tqdm(dataloader, desc="Evaluating")):
                if sample_count >= max_samples:
                    break

                pixel_values = batch['pixel_values'].to(device)
                image_ids = batch['image_ids']

                # Generate captions
                generated_captions = model.generate_caption(
                    pixel_values,
                    max_length=FIXED_CONFIG['MAX_GENERATION_LENGTH'],
                    do_sample=False
                )

                # Collect predictions and references
                for i, (gen_cap, img_id) in enumerate(zip(generated_captions, image_ids)):
                    # Get reference captions for this image
                    ref_captions = dataloader.dataset.get_image_captions(img_id)

                    predictions.append(gen_cap)
                    references.append(ref_captions)

                    predictions_dict[img_id] = gen_cap
                    references_dict[img_id] = ref_captions

                    sample_count += 1
                    if sample_count >= max_samples:
                        break

        print(f"Evaluated {sample_count} samples")

        # Compute all metrics
        print("Computing BLEU scores...")
        bleu_scores = self.compute_bleu_scores(predictions, references)

        print("Computing METEOR score...")
        meteor_score = self.compute_meteor_score(predictions, references)

        print("Computing ROUGE-L score...")
        rouge_l_score = self.compute_rouge_l_score(predictions, references)

        print("Computing CIDEr and SPICE scores...")
        coco_metrics = self.compute_coco_metrics(predictions_dict, references_dict)

        # Combine all metrics
        all_metrics = {
            **bleu_scores,
            'meteor': meteor_score,
            'rouge_l': rouge_l_score,
            **coco_metrics
        }

        return all_metrics, predictions[:10], references[:10]  # Return sample predictions

# Initialize evaluation metrics
evaluator = EvaluationMetrics()

# ============================================================================
# CELL 8: Training Loop with Early Stopping and Curves
# ============================================================================

# def train_model():
#     """
#     Main training function with early stopping, loss curves, and accuracy curves
#     """
#     print("=" * 60)
#     print("STARTING MODEL TRAINING")
#     print("=" * 60)

#     # Initialize model
#     model = ViTGPT2Model(
#         vit_model_name=CONFIG['VIT_MODEL'],
#         gpt2_model_name=CONFIG['GPT2_MODEL']
#     ).to(device)

#     # Count parameters
#     total_params = sum(p.numel() for p in model.parameters())
#     trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
#     print(f"Total parameters: {total_params:,}")
#     print(f"Trainable parameters: {trainable_params:,}")

#     # Initialize optimizer and scheduler
#     optimizer = torch.optim.AdamW(
#         model.parameters(),
#         lr=CONFIG['LEARNING_RATE'],
#         weight_decay=0.01,
#         betas=(0.9, 0.999)
#     )

#     num_training_steps = CONFIG['NUM_EPOCHS'] * len(train_dataloader)
#     num_warmup_steps = int(0.1 * num_training_steps)

#     scheduler = get_scheduler(
#         name="cosine",
#         optimizer=optimizer,
#         num_warmup_steps=num_warmup_steps,
#         num_training_steps=num_training_steps
#     )

# Fixed training function with better initialization
def train_model_fixed():
    """
    Fixed training function with better practices
    """
    print("Starting FIXED training...")

    # Initialize model
    model = ViTGPT2Model(
        vit_model_name=FIXED_CONFIG['VIT_MODEL'],
        gpt2_model_name=FIXED_CONFIG['GPT2_MODEL']
    ).to(device)

    # Freeze ViT encoder initially (optional - can help with stability)
    # for param in model.vit.parameters():
    #     param.requires_grad = False

    # Initialize optimizer with different learning rates
    optimizer = torch.optim.AdamW([
        {'params': model.vit.parameters(), 'lr': FIXED_CONFIG['LEARNING_RATE'] * 0.1},  # Lower LR for ViT
        {'params': model.gpt2.parameters(), 'lr': FIXED_CONFIG['LEARNING_RATE'] * 0.5},  # Lower LR for GPT2
        {'params': model.image_projection.parameters(), 'lr': FIXED_CONFIG['LEARNING_RATE']}  # Full LR for projection
    ], weight_decay=FIXED_CONFIG['WEIGHT_DECAY'])

    num_training_steps = FIXED_CONFIG['NUM_EPOCHS'] * len(train_dataloader)
    num_warmup_steps = int(FIXED_CONFIG['WARMUP_RATIO'] * num_training_steps)

    scheduler = get_scheduler(
        name="cosine_with_restarts",
        optimizer=optimizer,
        num_warmup_steps=num_warmup_steps,
        num_training_steps=num_training_steps
    )

    print(f"Training with {num_training_steps} total steps and {num_warmup_steps} warmup steps")
    # Training tracking variables
    train_losses = []
    val_losses = []
    learning_rates = []

    # Metric tracking for accuracy curves
    bleu_1_scores = []
    bleu_2_scores = []
    bleu_3_scores = []
    bleu_4_scores = []
    meteor_scores = []
    rouge_l_scores = []
    cider_scores = []
    spice_scores = []

    # Early stopping variables
    best_val_loss = float('inf')
    best_bleu4_score = 0.0
    patience_counter = 0
    best_epoch = 0

    print(f"Training configuration:")
    print(f"  Epochs: {FIXED_CONFIG['NUM_EPOCHS']}")
    print(f"  Batch size: {FIXED_CONFIG['BATCH_SIZE']}")
    print(f"  Learning rate: {FIXED_CONFIG['LEARNING_RATE']}")
    print(f"  Patience: {FIXED_CONFIG['PATIENCE']}")
    print(f"  Warmup steps: {num_warmup_steps}")
    print(f"  Total training steps: {num_training_steps}")

    # Training loop
    for epoch in range(FIXED_CONFIG['NUM_EPOCHS']):
        print(f"\n{'='*20} EPOCH {epoch+1}/{FIXED_CONFIG['NUM_EPOCHS']} {'='*20}")

        # =========================
        # TRAINING PHASE
        # =========================
        model.train()
        total_train_loss = 0.0
        num_train_batches = 0

        train_pbar = tqdm(train_dataloader, desc=f"Training Epoch {epoch+1}")

        for batch_idx, batch in enumerate(train_pbar):
            try:
                # Move data to device
                pixel_values = batch['pixel_values'].to(device)
                caption_tokens = batch['caption_tokens'].to(device)

                # Forward pass
                outputs = model(pixel_values=pixel_values, caption_tokens=caption_tokens)
                loss = outputs.loss

                # Backward pass
                loss.backward()

                # Gradient clipping
                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)

                # Update weights
                optimizer.step()
                scheduler.step()
                optimizer.zero_grad()

                # Track metrics
                total_train_loss += loss.item()
                num_train_batches += 1

                # Update progress bar
                current_lr = scheduler.get_last_lr()[0] if hasattr(scheduler, 'get_last_lr') else FIXED_CONFIG['LEARNING_RATE']
                train_pbar.set_postfix({
                    'loss': f"{loss.item():.4f}",
                    'lr': f"{current_lr:.2e}"
                })

            except Exception as e:
                print(f"Error in training batch {batch_idx}: {e}")
                continue

        # Calculate average training loss
        avg_train_loss = total_train_loss / max(num_train_batches, 1)
        train_losses.append(avg_train_loss)
        learning_rates.append(scheduler.get_last_lr()[0] if hasattr(scheduler, 'get_last_lr') else FIXED_CONFIG['LEARNING_RATE'])

        # =========================
        # VALIDATION PHASE
        # =========================
        model.eval()
        total_val_loss = 0.0
        num_val_batches = 0

        val_pbar = tqdm(val_dataloader, desc=f"Validation Epoch {epoch+1}")

        with torch.no_grad():
            for batch_idx, batch in enumerate(val_pbar):
                try:
                    # Move data to device
                    pixel_values = batch['pixel_values'].to(device)
                    caption_tokens = batch['caption_tokens'].to(device)

                    # Forward pass
                    outputs = model(pixel_values=pixel_values, caption_tokens=caption_tokens)
                    loss = outputs.loss

                    total_val_loss += loss.item()
                    num_val_batches += 1

                    val_pbar.set_postfix({'loss': f"{loss.item():.4f}"})

                except Exception as e:
                    print(f"Error in validation batch {batch_idx}: {e}")
                    continue

        # Calculate average validation loss
        avg_val_loss = total_val_loss / max(num_val_batches, 1)
        val_losses.append(avg_val_loss)

        # =========================
        # EVALUATION METRICS
        # =========================
        print(f"\nComputing evaluation metrics for epoch {epoch+1}...")

        try:
            # Evaluate on a subset of validation data for efficiency
            metrics, sample_preds, sample_refs = evaluator.evaluate_model(
                model, val_dataloader, max_samples=500
            )

            # Store metrics for curves
            bleu_1_scores.append(metrics['bleu_1'])
            bleu_2_scores.append(metrics['bleu_2'])
            bleu_3_scores.append(metrics['bleu_3'])
            bleu_4_scores.append(metrics['bleu_4'])
            meteor_scores.append(metrics['meteor'])
            rouge_l_scores.append(metrics['rouge_l'])
            cider_scores.append(metrics['cider'])
            spice_scores.append(metrics['spice'])

            # Print metrics
            print(f"\nEpoch {epoch+1} Results:")
            print(f"  Training Loss: {avg_train_loss:.4f}")
            print(f"  Validation Loss: {avg_val_loss:.4f}")
            print(f"  BLEU-1: {metrics['bleu_1']:.4f}")
            print(f"  BLEU-2: {metrics['bleu_2']:.4f}")
            print(f"  BLEU-3: {metrics['bleu_3']:.4f}")
            print(f"  BLEU-4: {metrics['bleu_4']:.4f}")
            print(f"  METEOR: {metrics['meteor']:.4f}")
            print(f"  ROUGE-L: {metrics['rouge_l']:.4f}")
            print(f"  CIDEr: {metrics['cider']:.4f}")
            print(f"  SPICE: {metrics['spice']:.4f}")

            # Print sample predictions
            print(f"\nSample Predictions:")
            for i in range(min(3, len(sample_preds))):
                print(f"  Generated: {sample_preds[i]}")
                print(f"  Reference: {sample_refs[i][0] if sample_refs[i] else 'N/A'}")
                print()

        except Exception as e:
            print(f"Error during evaluation: {e}")
            # Add default values if evaluation fails
            bleu_1_scores.append(0.0)
            bleu_2_scores.append(0.0)
            bleu_3_scores.append(0.0)
            bleu_4_scores.append(0.0)
            meteor_scores.append(0.0)
            rouge_l_scores.append(0.0)
            cider_scores.append(0.0)
            spice_scores.append(0.0)
            metrics = {'bleu_4': 0.0}

        # =========================
        # EARLY STOPPING LOGIC
        # =========================
        current_bleu4 = metrics.get('bleu_4', 0.0)

        # Save best model based on BLEU-4 score
        if current_bleu4 > best_bleu4_score:
            best_bleu4_score = current_bleu4
            best_val_loss = avg_val_loss
            best_epoch = epoch + 1
            patience_counter = 0

            # Save best model
            torch.save({
                'epoch': epoch + 1,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'scheduler_state_dict': scheduler.state_dict(),
                'best_bleu4': best_bleu4_score,
                'best_val_loss': best_val_loss,
                'train_losses': train_losses,
                'val_losses': val_losses,
                'metrics': metrics
            }, FIXED_CONFIG['MODEL_SAVE_PATH'])

            print(f"✓ Saved best model with BLEU-4: {best_bleu4_score:.4f}")

        else:
            patience_counter += 1
            print(f"No improvement for {patience_counter} epochs (best BLEU-4: {best_bleu4_score:.4f})")

            # Early stopping check
            if patience_counter >= FIXED_CONFIG['PATIENCE']:
                print(f"\nEarly stopping triggered after {epoch + 1} epochs!")
                print(f"Best epoch: {best_epoch} with BLEU-4: {best_bleu4_score:.4f}")
                break

    print(f"\nTraining completed!")
    print(f"Best model saved at epoch {best_epoch} with BLEU-4: {best_bleu4_score:.4f}")

    # Return training history
    training_history = {
        'train_losses': train_losses,
        'val_losses': val_losses,
        'learning_rates': learning_rates,
        'bleu_1_scores': bleu_1_scores,
        'bleu_2_scores': bleu_2_scores,
        'bleu_3_scores': bleu_3_scores,
        'bleu_4_scores': bleu_4_scores,
        'meteor_scores': meteor_scores,
        'rouge_l_scores': rouge_l_scores,
        'cider_scores': cider_scores,
        'spice_scores': spice_scores,
        'best_epoch': best_epoch,
        'best_bleu4': best_bleu4_score
    }

    return model, training_history

# ============================================================================
# CELL 9: Plotting Functions for Loss and Accuracy Curves
# ============================================================================

def plot_training_curves(training_history):
    """
    Plot comprehensive training curves
    This implements the "loss curve" and "accuracy curve" requirements
    """
    print("Plotting training curves...")

    # Create figure with subplots
    fig, axes = plt.subplots(3, 3, figsize=(20, 15))
    fig.suptitle('Training Progress and Evaluation Metrics', fontsize=16, fontweight='bold')

    epochs = range(1, len(training_history['train_losses']) + 1)

    # 1. Loss Curves
    axes[0, 0].plot(epochs, training_history['train_losses'], 'b-', label='Training Loss', linewidth=2)
    axes[0, 0].plot(epochs, training_history['val_losses'], 'r-', label='Validation Loss', linewidth=2)
    axes[0, 0].set_title('Training and Validation Loss', fontweight='bold')
    axes[0, 0].set_xlabel('Epoch')
    axes[0, 0].set_ylabel('Loss')
    axes[0, 0].legend()
    axes[0, 0].grid(True, alpha=0.3)

    # 2. Learning Rate
    axes[0, 1].plot(epochs, training_history['learning_rates'], 'g-', linewidth=2)
    axes[0, 1].set_title('Learning Rate Schedule', fontweight='bold')
    axes[0, 1].set_xlabel('Epoch')
    axes[0, 1].set_ylabel('Learning Rate')
    axes[0, 1].set_yscale('log')
    axes[0, 1].grid(True, alpha=0.3)

    # 3. BLEU Scores
    axes[0, 2].plot(epochs, training_history['bleu_1_scores'], 'purple', label='BLEU-1', linewidth=2)
    axes[0, 2].plot(epochs, training_history['bleu_2_scores'], 'orange', label='BLEU-2', linewidth=2)
    axes[0, 2].plot(epochs, training_history['bleu_3_scores'], 'brown', label='BLEU-3', linewidth=2)
    axes[0, 2].plot(epochs, training_history['bleu_4_scores'], 'red', label='BLEU-4', linewidth=2)
    axes[0, 2].set_title('BLEU Scores', fontweight='bold')
    axes[0, 2].set_xlabel('Epoch')
    axes[0, 2].set_ylabel('BLEU Score')
    axes[0, 2].legend()
    axes[0, 2].grid(True, alpha=0.3)

    # 4. METEOR Score
    axes[1, 0].plot(epochs, training_history['meteor_scores'], 'cyan', linewidth=2)
    axes[1, 0].set_title('METEOR Score', fontweight='bold')
    axes[1, 0].set_xlabel('Epoch')
    axes[1, 0].set_ylabel('METEOR Score')
    axes[1, 0].grid(True, alpha=0.3)

    # 5. ROUGE-L Score
    axes[1, 1].plot(epochs, training_history['rouge_l_scores'], 'magenta', linewidth=2)
    axes[1, 1].set_title('ROUGE-L Score', fontweight='bold')
    axes[1, 1].set_xlabel('Epoch')
    axes[1, 1].set_ylabel('ROUGE-L Score')
    axes[1, 1].grid(True, alpha=0.3)

    # 6. CIDEr Score
    axes[1, 2].plot(epochs, training_history['cider_scores'], 'navy', linewidth=2)
    axes[1, 2].set_title('CIDEr Score', fontweight='bold')
    axes[1, 2].set_xlabel('Epoch')
    axes[1, 2].set_ylabel('CIDEr Score')
    axes[1, 2].grid(True, alpha=0.3)

    # 7. SPICE Score
    axes[2, 0].plot(epochs, training_history['spice_scores'], 'darkgreen', linewidth=2)
    axes[2, 0].set_title('SPICE Score', fontweight='bold')
    axes[2, 0].set_xlabel('Epoch')
    axes[2, 0].set_ylabel('SPICE Score')
    axes[2, 0].grid(True, alpha=0.3)

    # 8. Combined Metrics Overview
    # Normalize metrics to 0-1 range for comparison
    def normalize_scores(scores):
        if not scores or max(scores) == 0:
            return scores
        return [s / max(scores) for s in scores]

    norm_bleu4 = normalize_scores(training_history['bleu_4_scores'])
    norm_meteor = normalize_scores(training_history['meteor_scores'])
    norm_rouge = normalize_scores(training_history['rouge_l_scores'])
    norm_cider = normalize_scores(training_history['cider_scores'])

    axes[2, 1].plot(epochs, norm_bleu4, label='BLEU-4 (norm)', linewidth=2)
    axes[2, 1].plot(epochs, norm_meteor, label='METEOR (norm)', linewidth=2)
    axes[2, 1].plot(epochs, norm_rouge, label='ROUGE-L (norm)', linewidth=2)
    axes[2, 1].plot(epochs, norm_cider, label='CIDEr (norm)', linewidth=2)
    axes[2, 1].set_title('Normalized Metrics Comparison', fontweight='bold')
    axes[2, 1].set_xlabel('Epoch')
    axes[2, 1].set_ylabel('Normalized Score')
    axes[2, 1].legend()
    axes[2, 1].grid(True, alpha=0.3)

    # 9. Training Summary
    axes[2, 2].axis('off')
    summary_text = f"""
    Training Summary:

    Best Epoch: {training_history['best_epoch']}
    Best BLEU-4: {training_history['best_bleu4']:.4f}

    Final Scores:
    BLEU-1: {training_history['bleu_1_scores'][-1]:.4f}
    BLEU-2: {training_history['bleu_2_scores'][-1]:.4f}
    BLEU-3: {training_history['bleu_3_scores'][-1]:.4f}
    BLEU-4: {training_history['bleu_4_scores'][-1]:.4f}
    METEOR: {training_history['meteor_scores'][-1]:.4f}
    ROUGE-L: {training_history['rouge_l_scores'][-1]:.4f}
    CIDEr: {training_history['cider_scores'][-1]:.4f}
    SPICE: {training_history['spice_scores'][-1]:.4f}

    Total Epochs: {len(training_history['train_losses'])}
    """
    axes[2, 2].text(0.1, 0.5, summary_text, fontsize=12, verticalalignment='center',
                    bbox=dict(boxstyle="round,pad=0.5", facecolor="lightgray", alpha=0.8))

    plt.tight_layout()
    plt.savefig('plots/training_curves.png', dpi=300, bbox_inches='tight')
    plt.savefig('plots/training_curves.pdf', bbox_inches='tight')
    plt.show()

    print("✓ Training curves saved to 'plots/training_curves.png' and 'plots/training_curves.pdf'")

def plot_loss_curves_only(training_history):
    """Plot detailed loss curves"""
    plt.figure(figsize=(12, 4))

    epochs = range(1, len(training_history['train_losses']) + 1)

    # Loss curves
    plt.subplot(1, 2, 1)
    plt.plot(epochs, training_history['train_losses'], 'b-', label='Training Loss', linewidth=2, marker='o')
    plt.plot(epochs, training_history['val_losses'], 'r-', label='Validation Loss', linewidth=2, marker='s')
    plt.title('Training and Validation Loss', fontsize=14, fontweight='bold')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.legend()
    plt.grid(True, alpha=0.3)

    # Loss difference
    plt.subplot(1, 2, 2)
    loss_diff = [v - t for t, v in zip(training_history['train_losses'], training_history['val_losses'])]
    plt.plot(epochs, loss_diff, 'g-', label='Val - Train Loss', linewidth=2, marker='^')
    plt.title('Overfitting Monitor', fontsize=14, fontweight='bold')
    plt.xlabel('Epoch')
    plt.ylabel('Loss Difference')
    plt.legend()
    plt.grid(True, alpha=0.3)
    plt.axhline(y=0, color='k', linestyle='--', alpha=0.5)

    plt.tight_layout()
    plt.savefig('plots/loss_curves_detailed.png', dpi=300, bbox_inches='tight')
    plt.show()

# ============================================================================
# CELL 10: Model Testing and Final Evaluation
# ============================================================================

def test_trained_model(model_path=None):
    """
    Test the trained model on test set and generate comprehensive evaluation
    """
    print("=" * 60)
    print("TESTING TRAINED MODEL")
    print("=" * 60)

    # Load the best model
    model_path = model_path or FIXED_CONFIG['MODEL_SAVE_PATH']

    if not os.path.exists(model_path):
        print(f"Model file not found: {model_path}")
        return None

    # Initialize model
    model = ViTGPT2Model(
        vit_model_name=FIXED_CONFIG['VIT_MODEL'],
        gpt2_model_name=FIXED_CONFIG['GPT2_MODEL']
    ).to(device)

    # Load trained weights
    checkpoint = torch.load(model_path, map_location=device)
    model.load_state_dict(checkpoint['model_state_dict'])
    print(f"✓ Loaded model from epoch {checkpoint['epoch']}")
    print(f"✓ Best BLEU-4 from training: {checkpoint['best_bleu4']:.4f}")

    # Evaluate on test set
    print("\nEvaluating on test set...")
    test_metrics, sample_preds, sample_refs = evaluator.evaluate_model(
        model, test_dataloader, max_samples=1000
    )

    # Print comprehensive results
    print(f"\n{'='*20} TEST SET RESULTS {'='*20}")
    print(f"BLEU-1:   {test_metrics['bleu_1']:.4f}")
    print(f"BLEU-2:   {test_metrics['bleu_2']:.4f}")
    print(f"BLEU-3:   {test_metrics['bleu_3']:.4f}")
    print(f"BLEU-4:   {test_metrics['bleu_4']:.4f}")
    print(f"METEOR:   {test_metrics['meteor']:.4f}")
    print(f"ROUGE-L:  {test_metrics['rouge_l']:.4f}")
    print(f"CIDEr:    {test_metrics['cider']:.4f}")
    print(f"SPICE:    {test_metrics['spice']:.4f}")

    # Generate sample predictions
    print(f"\n{'='*20} SAMPLE PREDICTIONS {'='*20}")
    for i in range(min(10, len(sample_preds))):
        print(f"\nSample {i+1}:")
        print(f"  Generated: {sample_preds[i]}")
        print(f"  Reference: {sample_refs[i][0] if sample_refs[i] else 'N/A'}")
        if len(sample_refs[i]) > 1:
            for j, ref in enumerate(sample_refs[i][1:], 2):
                print(f"  Reference {j}: {ref}")

    # Save test results
    test_results = {
        'test_metrics': test_metrics,
        'sample_predictions': sample_preds[:20],
        'sample_references': sample_refs[:20],
        'model_info': {
            'model_path': model_path,
            'epoch': checkpoint['epoch'],
            'best_training_bleu4': checkpoint['best_bleu4']
        }
    }

    with open('outputs/test_results.json', 'w') as f:
        json.dump(test_results, f, indent=2)

    print(f"\n✓ Test results saved to 'outputs/test_results.json'")

    return test_results

def generate_sample_captions(model_path=None, num_samples=5):
    """Generate captions for sample images"""
    print("Generating sample captions...")

    # Load model
    model_path = model_path or FIXED_CONFIG['MODEL_SAVE_PATH']
    model = ViTGPT2Model().to(device)
    checkpoint = torch.load(model_path, map_location=device)
    model.load_state_dict(checkpoint['model_state_dict'])
    model.eval()

    # Get sample images from test set
    sample_images = test_dataset.data['images'][:num_samples]

    plt.figure(figsize=(15, 3 * num_samples))

    for i, img_info in enumerate(sample_images):
        img_filename = img_info['file_name']
        img_path = os.path.join(FIXED_CONFIG['IMAGE_DIR'], img_filename)

        if os.path.exists(img_path):
            # Load and display image
            image = Image.open(img_path).convert("RGB")
            plt.subplot(num_samples, 2, 2*i + 1)
            plt.imshow(image)
            plt.title(f"Image: {img_filename}")
            plt.axis('off')

            # Generate caption
            pixel_values = image_processor(images=image, return_tensors="pt").pixel_values.to(device)
            generated_caption = model.generate_caption(pixel_values, max_length=50)[0]

            # Get reference captions
            img_id = img_info['id']
            ref_captions = test_dataset.get_image_captions(img_id)

            # Display captions
            plt.subplot(num_samples, 2, 2*i + 2)
            plt.axis('off')
            caption_text = f"Generated Caption:\n{generated_caption}\n\n"
            caption_text += "Reference Captions:\n"
            for j, ref in enumerate(ref_captions[:3], 1):
                caption_text += f"{j}. {ref}\n"

            plt.text(0.05, 0.95, caption_text, transform=plt.gca().transAxes,
                    fontsize=10, verticalalignment='top',
                    bbox=dict(boxstyle="round,pad=0.5", facecolor="lightblue", alpha=0.8))

    plt.tight_layout()
    plt.savefig('outputs/sample_captions.png', dpi=300, bbox_inches='tight')
    plt.show()

    print("✓ Sample captions saved to 'outputs/sample_captions.png'")

def test_generation():
    """Test if the model can generate non-empty captions"""
    model = ViTGPT2Model().to(device)

    # Create dummy image
    dummy_image = torch.randn(1, 3, 224, 224).to(device)

    # Generate caption
    captions = model.generate_caption(dummy_image, max_length=20, do_sample=True, temperature=0.8)

    print("Generated caption:", captions[0])
    print("Caption length:", len(captions[0]))

# ============================================================================
# CELL 11: Main Execution
# ============================================================================

def main():
    """Main execution function"""
    print("=" * 80)
    print("VISION TRANSFORMER + GPT-2 IMAGE CAPTIONING")
    print("=" * 80)

    # Check if required files exist
    required_files = ["rsicd_train.json", "rsicd_val.json", "rsicd_test.json"]
    missing_files = [f for f in required_files if not os.path.exists(f)]

    if missing_files:
        print(f"Missing required files: {missing_files}")
        print("Please run the dataset splitting cell first!")
        return

    # Train the model
    print("Starting training...")
    trained_model, training_history = train_model_fixed()

    # Plot training curves
    print("\nGenerating training curves...")
    plot_training_curves(training_history)
    plot_loss_curves_only(training_history)

    # Test the model
    print("\nTesting the trained model...")
    test_results = test_trained_model()

    # Generate sample captions
    print("\nGenerating sample captions...")
    generate_sample_captions(num_samples=5)

    # Save training history
    with open('outputs/training_history.json', 'w') as f:
        json.dump(training_history, f, indent=2)

    print("\n" + "=" * 80)
    print("TRAINING AND EVALUATION COMPLETED SUCCESSFULLY!")
    print("=" * 80)
    print("Files generated:")
    print("  - best_image_captioning_model.pth (best model)")
    print("  - plots/training_curves.png (comprehensive curves)")
    print("  - plots/loss_curves_detailed.png (detailed loss curves)")
    print("  - outputs/test_results.json (test evaluation)")
    print("  - outputs/training_history.json (training metrics)")
    print("  - outputs/sample_captions.png (sample predictions)")

    return trained_model, training_history, test_results

# Run the complete pipeline
if __name__ == "__main__":
    trained_model, training_history, test_results = main()

# ============================================================================
# CELL 12: Additional Utilities and Analysis
# ============================================================================

def analyze_training_results(training_history):
    """Analyze training results and provide insights"""
    print("=" * 60)
    print("TRAINING ANALYSIS")
    print("=" * 60)

    # Training stability analysis
    train_losses = training_history['train_losses']
    val_losses = training_history['val_losses']

    # Calculate improvement rates
    if len(train_losses) > 1:
        train_improvement = (train_losses[0] - train_losses[-1]) / train_losses[0] * 100
        val_improvement = (val_losses[0] - val_losses[-1]) / val_losses[0] * 100

        print(f"Loss Improvements:")
        print(f"  Training loss improved by: {train_improvement:.2f}%")
        print(f"  Validation loss improved by: {val_improvement:.2f}%")

    # Overfitting analysis
    final_gap = val_losses[-1] - train_losses[-1]
    avg_gap = np.mean([v - t for v, t in zip(val_losses, train_losses)])

    print(f"\nOverfitting Analysis:")
    print(f"  Final validation-training gap: {final_gap:.4f}")
    print(f"  Average validation-training gap: {avg_gap:.4f}")

    if final_gap > 0.5:
        print("  ⚠️  High overfitting detected")
    elif final_gap > 0.2:
        print("  ⚠️  Moderate overfitting detected")
    else:
        print("  ✓ Low overfitting - good generalization")

    # Best metrics summary
    best_bleu4_idx = np.argmax(training_history['bleu_4_scores'])
    best_meteor_idx = np.argmax(training_history['meteor_scores'])
    best_cider_idx = np.argmax(training_history['cider_scores'])

    print(f"\nBest Metric Epochs:")
    print(f"  Best BLEU-4: {training_history['bleu_4_scores'][best_bleu4_idx]:.4f} at epoch {best_bleu4_idx + 1}")
    print(f"  Best METEOR: {training_history['meteor_scores'][best_meteor_idx]:.4f} at epoch {best_meteor_idx + 1}")
    print(f"  Best CIDEr: {training_history['cider_scores'][best_cider_idx]:.4f} at epoch {best_cider_idx + 1}")

def compare_with_baselines():
    """Compare results with typical baselines"""
    print("\n" + "=" * 60)
    print("BASELINE COMPARISON")
    print("=" * 60)

    # Typical baseline scores for image captioning
    baselines = {
        'Random': {'bleu_4': 0.01, 'meteor': 0.05, 'cider': 0.10},
        'Template-based': {'bleu_4': 0.10, 'meteor': 0.15, 'cider': 0.30},
        'CNN-RNN (basic)': {'bleu_4': 0.18, 'meteor': 0.22, 'cider': 0.60},
        'CNN-LSTM (advanced)': {'bleu_4': 0.25, 'meteor': 0.27, 'cider': 0.85},
        'Transformer (basic)': {'bleu_4': 0.28, 'meteor': 0.29, 'cider': 0.95},
        'State-of-the-art': {'bleu_4': 0.35, 'meteor': 0.32, 'cider': 1.20}
    }

    print("Baseline Comparison (typical scores):")
    print(f"{'Method':<20} {'BLEU-4':<10} {'METEOR':<10} {'CIDEr':<10}")
    print("-" * 50)

    for method, scores in baselines.items():
        print(f"{method:<20} {scores['bleu_4']:<10.3f} {scores['meteor']:<10.3f} {scores['cider']:<10.3f}")

    print("\nNote: These are typical ranges. Actual performance depends on dataset and implementation.")

def save_model_summary():
    """Save a comprehensive model summary"""
    summary = {
        'model_architecture': 'ViT + GPT-2',
        'vit_model': FIXED_CONFIG['VIT_MODEL'],
        'gpt2_model': FIXED_CONFIG['GPT2_MODEL'],
        'training_config': FIXED_CONFIG,
        'dataset_info': {
            'train_samples': len(train_dataset),
            'val_samples': len(val_dataset),
            'test_samples': len(test_dataset)
        },
        'implementation_features': [
            'Early stopping based on BLEU-4 score',
            'Learning rate scheduling with warmup',
            'Gradient clipping for stability',
            'Comprehensive evaluation metrics',
            'Image feature pooling to handle position embeddings',
            'Loss and accuracy curve tracking'
        ]
    }

    with open('outputs/model_summary.json', 'w') as f:
        json.dump(summary, f, indent=2)

    print("✓ Model summary saved to 'outputs/model_summary.json'")

# ============================================================================
# CELL 13: Final Execution and Summary
# ============================================================================

# Execute analysis functions if training history is available
def final_analysis():
    """Run final analysis and cleanup"""
    try:
        if 'training_history' in globals():
            analyze_training_results(training_history)
            compare_with_baselines()
            save_model_summary()
        else:
            print("Training history not available. Run training first.")
    except Exception as e:
        print(f"Error in final analysis: {e}")

# Print execution summary
def print_execution_summary():
    """Print a summary of what has been implemented"""
    print("\n" + "=" * 80)
    print("IMPLEMENTATION SUMMARY")
    print("=" * 80)

    implemented_features = [
        "✓ Create JSON files when split - Dataset splitting into train/val/test with COCO format",
        "✓ Model (ViT + GPT2) - Vision Transformer encoder with GPT-2 decoder",
        "✓ Early stopping - Based on BLEU-4 score with configurable patience",
        "✓ Loss curve - Training and validation loss tracking and plotting",
        "✓ Accuracy curve - Multiple evaluation metrics tracked over epochs",
        "✓ Evaluate BLEU 1-4 - All BLEU variants computed using Hugging Face evaluate",
        "✓ Evaluate METEOR - METEOR metric using Hugging Face evaluate",
        "✓ Evaluate CIDEr - CIDEr metric using COCO evaluation tools",
        "✓ Evaluate ROUGE_L - ROUGE-L metric using Hugging Face evaluate",
        "✓ Evaluate SPICE - SPICE metric using COCO evaluation tools"
    ]

    for feature in implemented_features:
        print(feature)

    print("\nAdditional Features Implemented:")
    additional_features = [
        "• Comprehensive dataset class with proper COCO format handling",
        "• Position embedding fix using adaptive pooling",
        "• Gradient clipping and learning rate scheduling",
        "• Detailed training progress visualization",
        "• Sample caption generation with visual examples",
        "• Model checkpointing and best model saving",
        "• Overfitting analysis and training stability metrics",
        "• Error handling and robust training pipeline",
        "• Configurable hyperparameters",
        "• Comprehensive logging and result saving"
    ]

    for feature in additional_features:
        print(feature)

    print("\nGenerated Files:")
    output_files = [
        "• rsicd_train.json, rsicd_val.json, rsicd_test.json - Dataset splits",
        "• best_image_captioning_model.pth - Best trained model",
        "• plots/training_curves.png - Comprehensive training visualization",
        "• plots/loss_curves_detailed.png - Detailed loss analysis",
        "• outputs/test_results.json - Final evaluation results",
        "• outputs/training_history.json - Complete training metrics",
        "• outputs/sample_captions.png - Visual caption examples",
        "• outputs/model_summary.json - Model and training summary"
    ]

    for file_info in output_files:
        print(file_info)

    print("\n" + "=" * 80)
    print("ALL REQUIREMENTS SUCCESSFULLY IMPLEMENTED!")
    print("=" * 80)

# Run final analysis
final_analysis()

# Print execution summary
print_execution_summary()

# ============================================================================
# END OF COMPLETE IMPLEMENTATION
# ============================================================================

